#Import dependency package
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split,cross_validate,GridSearchCV, KFold
from sklearn.dummy import DummyRegressor
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler, MinMaxScaler, MaxAbsScaler, RobustScaler, LabelEncoder, OrdinalEncoder, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import make_pipeline
from sklearn.linear_model import Lasso,BayesianRidge,Lasso,LinearRegression
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn.neighbors import KNeighborsRegressor
from sklearn.svm import SVR
import xgboost as xgb
from xgboost import XGBRegressor
from pickletools import float8
from scipy import stats
import shap
import itertools
from cycler import cycler
from sklearn.metrics import mean_squared_error, r2_score,accuracy_score
import optuna
from lime.lime_tabular import LimeTabularExplainer
from sklearn.decomposition import PCA
import warnings
warnings.filterwarnings("ignore")

#Import data
data = pd.read_excel('data/2090.xlsx')

#Data preprocessing
data.duplicated().sum()
data = data.drop_duplicates(subset=None, keep='first', inplace=False)
data = data.dropna(subset= 'HRA')
data.columns
X = data.drop(columns=[ 'HRA','KIC (MPa. m1/2)', 'TRS'])
y = data['HRA']
X_train, X_test, y_train, y_test = train_test_split(X,y, train_size=0.8, random_state=50)
X_train[['WC', 'Co', 'TiC', 'TaC/NbC', 'VC', 'Cr3C2', 'ZrC']] = X_train[['WC', 'Co', 'TiC', 'TaC/NbC', 'VC', 'Cr3C2', 'ZrC']].fillna(0)
num_data = X_train.select_dtypes(include=float).columns.tolist()
cat_data = X_train.select_dtypes(exclude=np.number).columns.tolist()
num_pipe = make_pipeline(SimpleImputer(strategy='most_frequent'), StandardScaler())
cat_pipe = make_pipeline(SimpleImputer(strategy='most_frequent'), OrdinalEncoder(handle_unknown = 'use_encoded_value', unknown_value = -2))
complete_pipe = ColumnTransformer([('num', num_pipe, num_data), ('cat', cat_pipe, cat_data)])

#Build model
knn_model = KNeighborsRegressor(n_neighbors=4, p=1, weights='distance')
random_model = RandomForestRegressor(bootstrap=False, max_depth=7, max_features='sqrt', min_samples_leaf=5, min_samples_split=7, n_estimators=90)
svr_model = SVR(C=3,kernel='linear', degree=1, gamma=0.1,coef0=0.0)
xgboost_model = XGBRegressor(booster='gbtree',n_estimators=180, learning_rate=0.01,  max_depth=4, min_child_weight=0.5, seed=40)

#Training model
final_pipe = make_pipeline(complete_pipe,random_model)
final_pipe.fit(X_train, y_train)

#使用r2评估模型
cv_results = cross_validate(final_pipe, X_train, y_train, cv=10, scoring='r2', return_train_score=True)
print("R2 Scores:", cv_results['test_score'])
print("Average R2 Score:", np.mean(cv_results ['test_score']))
r2_variance = np.var(cv_results['test_score'])
y_pred = final_pipe.predict(X_test)
print("R2 Score Variance:", r2_variance)

#网格搜索调参
n_estimators_range = range(20, 200, 10)
max_depth_range = range(1, 20,1)
max_features_range = range(3, 10)
param_grid = {
    'n_estimators': n_estimators_range,
    'max_depth': max_depth_range,
    'max_features' : max_features_range
}
rf = RandomForestRegressor()
scoring = {'r2': 'r2'}
grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=5, scoring=scoring, refit='r2',return_train_score=True)
grid_search.fit(X_train, y_train)
results_rf = pd.DataFrame(grid_search.cv_results_)
results_rf = results_rf[['param_n_estimators', 'param_max_depth','param_max_features', 'mean_test_r2']]
results_rf.columns = ['n_estimators', 'max_depth', 'max_features','r2_valid']
results_rf.loc[:, 'n_estimators'] = results_rf['n_estimators'].astype(int)
results_rf.loc[:, 'max_depth'] = results_rf['max_depth'].astype(int)
results_rf.loc[:, 'max_features'] = results_rf['max_depth'].astype(int)
print(results_rf)
sns.set(font_scale = 1)
arr_test_heatmap = results_rf.pivot(index="n_estimators", columns="max_depth", values="r2_valid")
#ax_test = sns.heatmap(arr_test_heatmap, data=results_rf[['r2_valid']], cmap = "Blues", vmin=0.4, vmax=0.9)
ax_test = sns.heatmap(arr_test_heatmap, cmap = "Blues", vmin=0.6, vmax=0.9)
print("\nr2 heatmap:")
plt.show()

#使用3q工具检测异常值，筛选数据
data=pd.read_excel('data/2090.xlsx')
df=pd.DataFrame(data,columns=['Cr3C2'])
u = df['Cr3C2'].mean()
std = df['Cr3C2'].std()
p=stats.kstest(df,'norm',(u,std))
print(stats.kstest(df,'norm',(u,std)))
print('The average is:%.3f，The standard deviation is:：%.3f'%(u,std))
print('-------')
error = df[np.abs(df['Cr3C2']-u)>3*std]
data_c = df[np.abs(df['Cr3C2']-u)<= 3*std]
print("Output normal data")
print(data_c)
print("Output exception data")
print(error)

#画热力图
plt.rcParams['axes.unicode_minus'] = False
corr = X_train.corr(method='pearson')
print(corr)
correlation_matrix = X_train.corr()
mask = np.tri(len(correlation_matrix), k=-1)
np.fill_diagonal(mask, False)
fig, ax = plt.subplots(1,1,figsize=(16,12))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f", linewidths=0.5, mask=mask)

#绘制SHAP图
data = pd.read_excel('data/2090.xlsx')
data = data.dropna(subset='HRA') 
data = data.dropna(subset='grain size')
X = data.drop(columns=['HRA','KIC (MPa. m1/2)','TRS'])
y = data['HRA']
X[['WC', 'Co', 'TiC', 'TaC/NbC', 'VC', 'Cr3C2', 'ZrC']] = X[['WC', 'Co', 'TiC', 'TaC/NbC', 'VC', 'Cr3C2', 'ZrC']].fillna(0)
X_train, X_test, y_train, y_test = train_test_split(X,y, train_size=0.8, random_state=30)
rf_model = RandomForestRegressor(bootstrap=False, max_depth=7, max_features='sqrt', min_samples_leaf=5, min_samples_split=5, n_estimators=60)
rf_model.fit(X_train, y_train)
y_pred = rf_model.predict(X_test)
mse = mean_squared_error(y_test, y_pred)**(0.5)
explainer = shap.TreeExplainer(rf_model)
print (explainer.expected_value)
shap_values = explainer.shap_values(X_train)
shap.force_plot(explainer.expected_value, shap_values[1],matplotlib = True,feature_names=X_train.columns)
shap.summary_plot(shap_values,X_train,feature_names=X_train.columns,plot_type='violin')
shap.summary_plot(shap_values,X_train,feature_names=X_train.columns)

#绘制散点图
preds = final_pipe.predict(X_test)
plt.scatter(y_test, preds, c='b', marker='o',alpha=1, s=10)
x = np.linspace(83,97)
y = x 
plt.plot(x,y, color='r',alpha=0.6)
plt.xlabel('Actual Values',fontsize=12);plt.ylabel('Predicted Values',fontsize=12)
plt.title('Scatter plot of Actual vs Predicted Values')

#绘制LIME图
X_train = np.array(X_train) 
X_test = np.array(X_test)
LIMEdata = pd.read_excel('data/LIME.xlsx')
LIMEdata = LIMEdata.iloc[:,0:8]
LIMEdata = LIMEdata.fillna(0)
scaler=StandardScaler()
scaler.fit(LIMEdata,X_train)
LIMEdata = scaler.transform(LIMEdata)
X_train = scaler.transform(X_train)
imputer = SimpleImputer(missing_values=np.nan, strategy='mean')
LIMEdata = imputer.fit_transform(LIMEdata)
X_train = imputer.fit_transform(X_train)
columns = ['WC', 'Co', 'TiC', 'TaC/NbC', 'VC', 'Cr3C2', 'ZrC','grain size']
LIMEdata = pd.DataFrame(LIMEdata,columns=columns)
X_train = pd.DataFrame(X_train,columns=columns)
explainer = LimeTabularExplainer(X_train.values, feature_names = columns, mode='regression')
np.random.seed(42)
explaination = explainer.explain_instance(LIMEdata.iloc[0], random_model.predict, num_features=8)
explaination.show_in_notebook(show_table = True, show_all = False)

#PCA降维
array = np.array(data)
pca = PCA(n_components=2)
pca.fit(data)
var_ratio = pca.explained_variance_ratio_
for idx, val in enumerate(var_ratio, 1):print("Principle component %d: %.2f%%" % (idx, val * 100))
print("total: %.2f%%" % np.sum(var_ratio * 100))
feature_importance = np.abs(pca.components_)
feature_importance_sum = np.sum(feature_importance, axis=0)
print("\n features importance（Contribution degree）:")
ranking_df = pd.DataFrame({'特征': data.columns, 'Contribution degree': feature_importance_sum})
ranking_df = ranking_df.sort_values(by='Contribution degree')
print(ranking_df)
df = pd.DataFrame(ranking_df)
df.to_excel("Contribution degree.xlsx")

#利用排列组合生成特征数据
num = np.arange(60,97,3) 
num1 = np.arange(3,30,2)
num2 = np.arange(0,15,1)
num3 = np.arange(0,5,1)
num4 = np.arange(0,5,1)
num5 = np.arange(0,5,1)
num6 = np.arange(0,10,1)
num7 = np.arange(0.1,3,0.3)
data1 = [round(i,1) for i in list(num)] 
data2 = [round(i,1) for i in list(num1)] 
data3 = [round(i,1) for i in list(num2)] 
data4 = [round(i,1) for i in list(num3)] 
data5 = [round(i,1) for i in list(num4)] 
data6 = [round(i,1) for i in list(num5)] 
data7 = [round(i,1) for i in list(num6)] 
data8 = [round(i,1) for i in list(num7)] 
result = list(itertools.product(data1,data2,data3,data4,data5,data6,data7,data8))
result = [(a,b,c,d,e,f,g,h) for a,b,c,d,e,f,g,h in result if ( a+b+c+d+e+f+g == 100)]
df = pd.DataFrame(result,columns=['WC', 'Co', 'TiC', 'TaC/NbC', 'VC', 'Cr3C2', 'ZrC', 'grain size'])
df.to_excel('output.xlsx', index=False)

#预测性能
data_test = pd.read_excel("output.xlsx")
test = final_pipe.predict(data_test)
data_test["HRA"] = pd.DataFrame({'HRA': test})
data_test.to_excel('output_HRA.xlsx', index=False)
